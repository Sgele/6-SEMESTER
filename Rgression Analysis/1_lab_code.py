# -*- coding: utf-8 -*-
"""Binarinis_modelis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YT6hdsJiSC9AqBcNLvcAF7I3UODKXfjG
"""

import pandas as pd                     
import matplotlib.pyplot as plt         
import numpy as np                       
from scipy.sparse import csr_matrix

df1 = pd.read_excel("train_data.xlsx")
df1

y = df1["Outcome"]
print(y.nunique())
y.value_counts()

from yellowbrick.regressor import CooksDistance
from yellowbrick.datasets import load_concrete
# ISSKIRTYS - kuko matas
x = df1[['Pregnancies', 'Glucose',	'BloodPressure',	'SkinThickness',	'Insulin',	'BMI',	'DiabetesPedigreeFunction', "Age"]]
y = df1['Outcome']

# Instantiate and fit the visualizer
visualizer = CooksDistance()
visualizer.fit(x, y)
visualizer.show()

# multikolinearumo patikrinimui

M = ["Pregnancies","Glucose","BloodPressure","SkinThickness","Insulin","BMI", "DiabetesPedigreeFunction", "Age", "Outcome"]

corr_matrix = df1.loc[:,M].corr()
print(corr_matrix)

# vidutiniskai stipri koreliacija tarp nestumo ir amziaus

from statsmodels.stats.outliers_influence import variance_inflation_factor
from patsy import dmatrices


y, X = dmatrices('Outcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + Insulin + BMI + DiabetesPedigreeFunction + Age', data=df1, return_type='dataframe')

vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif["features"] = X.columns
vif.round(2)

# multikolinearumo nera, nes nevirsijo 4

# logit modelis

from statsmodels.genmod.generalized_linear_model import GLM
from statsmodels.genmod import families

x = df1[['Pregnancies', 'Glucose',	'BloodPressure',	'SkinThickness',	'Insulin',	'BMI',	'DiabetesPedigreeFunction', "Age"]]
y = df1['Outcome']

# Add constant
X_constant = sm.add_constant(x, prepend=False)
  
# Build model and fit the data (using statsmodel's Logit)
logit_results = GLM(y, X_constant, family=families.Binomial()).fit()

# Display summary results
print(logit_results.summary())
print(logit_results.aic)
print(logit_results.bic)

# ismetam skin thickness
x = df1[['Pregnancies', 'Glucose', 'BloodPressure',	'Insulin',	'BMI',	'DiabetesPedigreeFunction', "Age"]]
y = df1['Outcome']

# Add constant
X_constant = sm.add_constant(x, prepend=False)
  
# Build model and fit the data (using statsmodel's Logit)
logit_results = GLM(y, X_constant, family=families.Binomial()).fit()

# Display summary results
print(logit_results.summary())
print(logit_results.aic)
print(logit_results.bic)

# ismetam blood pressure
x = df1[['Pregnancies', 'Glucose',	'Insulin',	'BMI',	'DiabetesPedigreeFunction', "Age"]]
y = df1['Outcome']

# Add constant
X_constant = sm.add_constant(x, prepend=False)
  
# Build model and fit the data (using statsmodel's Logit)
logit_results = GLM(y, X_constant, family=families.Binomial()).fit()

# Display summary results
print(logit_results.summary())
print(logit_results.aic)
print(logit_results.bic)

# ismetam pregnancies
x = df1[[ 'Glucose',	'Insulin', 'BMI',	'DiabetesPedigreeFunction', "Age"]]
y = df1['Outcome']

# Add constant
X_constant = sm.add_constant(x, prepend=False)
  
# Build model and fit the data (using statsmodel's Logit)
logit_results = GLM(y, X_constant, family=families.Binomial()).fit()

# Display summary results
print(logit_results.summary())
print(logit_results.aic)
print(logit_results.bic)

# ismetam insulin
x = df1[[ 'Glucose','BMI',	'DiabetesPedigreeFunction', "Age"]]
y = df1['Outcome']

# Add constant
X_constant = sm.add_constant(x, prepend=False)
  
# Build model and fit the data (using statsmodel's Logit)
logit_results = GLM(y, X_constant, family=families.Binomial()).fit()

# Display summary results
print(logit_results.summary())
print(logit_results.aic)
print(logit_results.bic)

import math

# const = -9.8647 
print("Intercept: ", math.exp(-9.8647))

# glukoze = 0.0363
print("Glucose: ", math.exp(0.0363))

# BMI =  0.0596
print("BMI: ", math.exp(0.0596)) 

# DiabetesPedigreeFunction  =  0.8185
print("DiabetesPedigreeFunction: ", math.exp(0.05 * 0.8185)) 

# Age = 0.0613
print("Age: ", math.exp(0.0613))

# ismetam diabeto
x = df1[[ 'Glucose','BMI',	"Age"]]
y = df1['Outcome']

# Add constant
X_constant = sm.add_constant(x, prepend=False)
  
# Build model and fit the data (using statsmodel's Logit)
logit_results = GLM(y, X_constant, family=families.Binomial()).fit()

# Display summary results
print(logit_results.summary())
print(logit_results.aic)
print(logit_results.bic)

# paklaidu nepriklausomumas

from statsmodels.stats.stattools import durbin_watson as dwtest
import numpy as np



# Generate residual series plot
fig = plt.figure(figsize=(8,5))
ax = fig.add_subplot(111, title="Residual Series Plot",
                    xlabel="Index Number", ylabel="Deviance Residuals")

#ax.plot(X.index.tolist(), stats.zscore(logit_results.resid_pearson))
ax.plot(X.index.tolist(), stats.zscore(logit_results.resid_deviance), ls = "None", marker = ".")
plt.axhline(y=0, ls="--", color='red')

dwtest(logit_results.resid_deviance)

from sklearn.linear_model import LogisticRegression
df2 = pd.read_excel("test_data.xlsx")

#train data
x = df1[[ 'Glucose','BMI', 'DiabetesPedigreeFunction', "Age"]]
y = df1['Outcome']
# test data
x1 = df2[[ 'Glucose','BMI', 'DiabetesPedigreeFunction', "Age"]]
y1 = df2['Outcome']

logisticRegr = LogisticRegression()

logisticRegr.fit(x, y)
y_pred = logisticRegr.predict(x1)

from sklearn import metrics

cnf_matrix = metrics.confusion_matrix(y1, y_pred)
cnf_matrix

TP = cnf_matrix[0, 0]
FN = cnf_matrix[0, 1]
FP = cnf_matrix[1, 0]
TN = cnf_matrix[1, 1]


print("Jautrumas: ", TP/(TP + FN))
print("Specifiškumas: ", TN/(TN+FP))
print("Tikslumas: ", TP/(TP+FP))
print("Neigiama prognozuojama reikšmė: ", TN/(TN+FN))
print("Bendras tikslumas: ", (TP + TN)/(TP + TN + FP + FN))

y_pred_proba = logisticRegr.predict_proba(x1)[::,1]
y_pred_proba
fpr, tpr, _ = metrics.roc_curve(y1, y_pred_proba)
auc = metrics.roc_auc_score(y1, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()